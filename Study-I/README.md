# Experiment I: AI Generated Detection Likelihood Scoring

## Contents

- `requirements.txt` - for installing python package dependencies
- `experiment.py` - Python script to reproduce the experiment data, without needing web/desktop Auditomatic app
- `analysis.ipynb` - Original Experiment Data Analysis

### Note: 

#### `experiment.py` will generate:
- `results_YYYYMMDD_HHMMSS.db` - sqlite3 db for storing results and managing execution
- `results_YYYYMMDD_HHMMSS.csv` - generated by default after trial is complete

`analysis.ipynb` has outputs for our original experiment run, a new set of data can be generated using the `experiment.py` script. The analysis notebook can be run with the new data by changing the `data/results.csv` filepath in the beginning of the notebook to the resulting `results_YYYYMMDD_HHMMSS.csv` file

To successfully get a new set of data, you need an OpenRouter API key, and a local installation of [Ollama](https://ollama.com/) with `gpt-oss:20b` and `qwen3:14b`.

---

## Running a new Trial (create new data)

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

This installs all required packages plus optional dependencies for Excel and Parquet export.

### 2. Set API Keys (Required)

API keys **must** be set as environment variables:

```bash
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export OPENROUTER_API_KEY="sk-or-..."
# Add any other providers your experiment uses
```

**Or on Windows Powershell:**
```powershell
$env:OPENAI_API_KEY="sk-..."
$env:ANTHROPIC_API_KEY="sk-ant-..."
$env:OPENROUTER_API_KEY="sk-or-..."
```

The script checks all required keys upfront and will exit if any are missing.

### 3. Run the Experiment

**Basic usage:**
```bash
python experiment.py
```

---

## Command-Line Options

| Option                 | Default| Description |
|------------------------|--------|------------------------------------------------------------------|
| `--output` / `-o`      | `csv`  | Output format: `csv`, `tsv`, `json`, `jsonl`, `excel`, `parquet` |
| `--concurrent` / `-c`  | `10`   | Number of concurrent API requests |
| `--rate-limit` / `-r`  | `5.0`  | Maximum requests per second |
| `--timeout` / `-t`     | `90`   | Request timeout in seconds |
| `--output-file` / `-f` | Auto   | Custom output filename |
| `--resume`             | Off    | Resume from existing database |
| `--db-file`            | Auto   | Database file to use/resume from |

**Examples:**

```bash
# Fast batch processing
python experiment.py --concurrent 20 --rate-limit 10.0

# Conservative (avoid rate limits)
python experiment.py --concurrent 1 --rate-limit 1.0

# Long-running API calls
python experiment.py --timeout 180

# Resume from previous run
python experiment.py --resume --db-file results_20250128_143022.db

# Export to specific format
python experiment.py --output excel --output-file my_results.xlsx
```

**With concurrency and rate limiting (enabled by default):**
```bash
python experiment.py --concurrent 10 --rate-limit 5.0
```
Concurrency is how many simultaneous/parallel API requests are made at a time.
Rate limits are how many completed requests are made per second, on average.
The script tracks both and lets you set limits independently. The default is 10
concurrent requests, max of 5 requests per second.

This command runs 30 API calls concurrently (in parallel), with an additional
maximum of 10 completed requests per second:

```bash
python experiment.py --concurrent 30 --rate-limit 10.0
```

Or if you are using ollama for a local model on a GPU that cannot handle many
parallel requests, you can set the concurrency to 1 for serial/one-at-a-time mode:

```bash
python experiment.py --concurrent 1 --rate-limit 10.0
```

**With custom output format:**
By default, it saves results to the sqlite3 db file and exports to CSV on completion.
You can do:
```bash
python experiment.py --output excel
python experiment.py --output parquet
```

---

## Features

### Reliability

**Retry Logic:**
- Up to 10 automatic retry attempts with exponential backoff (1s â†’ 30s)
- Smart handling of rate limits (429) and server errors (500/502/503/504)
- Immediate failure on auth errors (401/403) - no wasted retries
- Detailed logging of retry attempts

**SQLite Persistence:**
- All results saved to SQLite database in real-time
- Atomic transactions ensure no data loss
- Query results with standard SQL tools
- Database survives crashes and interruptions

### Resume from Interruptions

Press Ctrl+C at any time. To resume you must specify the db generated:

```bash
python experiment.py --resume --db-file results_20250128_143022.db
```

The script will:
- Skip all previously successful calls
- Retry any previously failed calls
- Continue from where you left off
- Export with the updated results

### Smart Output Schema

**Dynamic columns based on your configuration:**
- Each parameter gets its own column: `param_temperature`, `param_max_tokens`, etc.
- Each variable gets its own column
- Full request/response payloads saved as JSON
- Extracted answers in dedicated column

**Example CSV structure:**
```
config_index | model_display_name | param_temperature | param_max_tokens | variable1 | variable2 | extracted | success | error
```

---

## Troubleshooting

### Rate Limit Errors (429)
```
Retry 3 for GPT-4 after HTTPStatusError: 429 Rate Limit
```
**Fix:** Reduce `--concurrent` or `--rate-limit`, or wait and use `--resume`

### Extraction Failures
```
[WARN] GPT-4: Extraction failed. Tried: choices[0].message.content, data.content
```
**Fix:** Check the API response format in the database (`response` column) and update `extract_paths` in the script

### Network Timeouts
```
Network error GPT-4: TimeoutException
```
**Fix:** Increase `--timeout` or check your network connection